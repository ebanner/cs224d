{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D Assignment #2\n",
    "# Part [1]: Deep Networks: NER Window Model\n",
    "\n",
    "For this first part of the assignment, you'll build your first \"deep\" networks. On problem set 1, you computed the backpropagation gradient $\\frac{\\partial J}{\\partial w}$ for a two-layer network; in this problem set you'll implement a slightly more complex network to perform  named entity recognition (NER).\n",
    "\n",
    "Before beginning the programming section, you should complete parts (a) and (b) of the corresponding section of the handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c): Random Initialization Test\n",
    "Use the cell below to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46994114 -0.83008197  0.23148553  0.43094097 -0.00258593]\n",
      " [-0.47666619 -0.52297046  0.45125243 -0.57311684 -0.71301636]\n",
      " [ 0.32105262  0.78530031 -0.85918681  0.02111762  0.54147539]]\n"
     ]
    }
   ],
   "source": [
    "from misc import random_weight_matrix\n",
    "random.seed(10)\n",
    "print random_weight_matrix(3,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d): Implementation\n",
    "\n",
    "We've provided starter code to load in the dataset and convert it to a list of \"windows\", consisting of indices into the matrix of word vectors. \n",
    "\n",
    "We pad each sentence with begin and end tokens `<s>` and `</s>`, which have their own word vector representations; additionally, we convert all words to lowercase, canonicalize digits (e.g. `1.12` becomes `DG.DGDG`), and replace unknown words with a special token `UUUNKKK`.\n",
    "\n",
    "You don't need to worry about the details of this, but you can inspect the `docs` variables or look at the raw data (in plaintext) in the `./data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_utils.utils as du\n",
    "import data_utils.ner as ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the starter word vectors\n",
    "wv, word_to_num, num_to_word = ner.load_wv('data/ner/vocab.txt',\n",
    "                                           'data/ner/wordVectors.txt')\n",
    "tagnames = [\"O\", \"LOC\", \"MISC\", \"ORG\", \"PER\"]\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = du.invert_dict(num_to_tag)\n",
    "\n",
    "# Set window size\n",
    "windowsize = 3\n",
    "\n",
    "# Load the training set\n",
    "docs = du.load_dataset('data/ner/train')\n",
    "X_train, y_train = du.docs_to_windows(docs, word_to_num, tag_to_num,\n",
    "                                      wsize=windowsize)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/ner/dev')\n",
    "X_dev, y_dev = du.docs_to_windows(docs, word_to_num, tag_to_num,\n",
    "                                  wsize=windowsize)\n",
    "\n",
    "# Load the test set (dummy labels only)\n",
    "docs = du.load_dataset('data/ner/test.masked')\n",
    "X_test, y_test = du.docs_to_windows(docs, word_to_num, tag_to_num,\n",
    "                                    wsize=windowsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid re-inventing the wheel, we provide a base class that handles a lot of the drudgery of managing parameters and running gradient descent. It's based on the classifier API used by [`scikit-learn`](http://scikit-learn.org/stable/), so if you're familiar with that library it should be easy to use. \n",
    "\n",
    "We'll be using this class for the rest of this assignment, so it helps to get acquainted with a simple example that should be familiar from Assignment 1. To keep this notebook uncluttered, we've put the code in the `softmax_example.py`; take a look at it there, then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/db error norm = 3.565e-10 [ok]\n",
      "    b dims: [5] = 5 elem\n",
      "grad_check: dJ/dW error norm = 2.164e-11 [ok]\n",
      "    W dims: [5, 100] = 500 elem\n",
      "grad_check: dJ/dL[5] error norm = 2.646e-11 [ok]\n",
      "    L[5] dims: [100] = 100 elem\n"
     ]
    }
   ],
   "source": [
    "from softmax_example import SoftmaxRegression\n",
    "sr = SoftmaxRegression(wv=zeros((10,100)), dims=(100,5))\n",
    "\n",
    "##\n",
    "# Automatic gradient checker!\n",
    "# this checks anything you add to self.grads or self.sgrads\n",
    "# using the method of Assignment 1\n",
    "sr.grad_check(x=5, y=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement a model, you need to subclass `NNBase`, then implement the following methods:\n",
    "\n",
    "- `__init__()` (initialize parameters and hyperparameters)\n",
    "- `_acc_grads()` (compute and accumulate gradients)\n",
    "- `compute_loss()` (compute loss for a training example)\n",
    "- `predict()`, `predict_proba()`, or other prediction method (for evaluation)\n",
    "\n",
    "`NNBase` provides you with a few others that will be helpful:\n",
    "\n",
    "- `grad_check()` (run a gradient check - calls `_acc_grads` and `compute_loss`)\n",
    "- `train_sgd()` (run SGD training; more on this later)\n",
    "\n",
    "Your task is to implement the window model in `nerwindow.py`; a scaffold has been provided for you with instructions on what to fill in.\n",
    "\n",
    "When ready, you can test below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/db2 error norm = 3.223e-10 [ok]\n",
      "    b2 dims: [5] = 5 elem\n",
      "grad_check: dJ/dU error norm = 2.838e-10 [ok]\n",
      "    U dims: [5, 100] = 500 elem\n",
      "grad_check: dJ/db1 error norm = 2.851e-09 [ok]\n",
      "    b1 dims: [100] = 100 elem\n",
      "grad_check: dJ/dW error norm = 1.337e-08 [ok]\n",
      "    W dims: [100, 150] = 15000 elem\n",
      "grad_check: dJ/dL[[   30  6659 12637]] error norm = 6.837e-11 [ok]\n",
      "    L[[   30  6659 12637]] dims: [3, 50] = 150 elem\n"
     ]
    }
   ],
   "source": [
    "from nerwindow import WindowMLP\n",
    "clf = WindowMLP(wv, windowsize=windowsize, dims=[None, 100, 5],\n",
    "                reg=0.001, alpha=0.01)\n",
    "\n",
    "clf.grad_check(X_train[0], y_train[0]) # gradient check on single point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train your model on some data! You can implement your own SGD method, but we recommend that you just call `clf.train_sgd`. This takes the following arguments:\n",
    "\n",
    "- `X`, `y` : training data\n",
    "- `idxiter`: iterable (list or generator) that gives index (row of X) of training examples in the order they should be visited by SGD\n",
    "- `printevery`: int, prints progress after this many examples\n",
    "- `costevery`: int, computes mean loss after this many examples. This is a costly operation, so don't make this too frequent!\n",
    "\n",
    "The implementation we give you supports minibatch learning; if `idxiter` is a list-of-lists (or yields lists), then gradients will be computed for all indices in a minibatch before modifying the parameters (this is why we have you write `_acc_grad` instead of applying them directly!).\n",
    "\n",
    "Before training, you should generate a training schedule to pass as `idxiter`. If you know how to use Python generators, we recommend those; otherwise, just make a static list. Make the following in the cell below:\n",
    "\n",
    "- An \"epoch\" schedule that just iterates through the training set, in order, `nepoch` times.\n",
    "- A random schedule of `N` examples sampled with replacement from the training set.\n",
    "- A random schedule of `N/k` minibatches of size `k`, sampled with replacement from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_size = len(y_train)\n",
    "\n",
    "def epoch_trainer(N, k=None):\n",
    "    \"\"\"Yields train indices that trains over n epochs\"\"\"\n",
    "    \n",
    "    for _ in range(N):\n",
    "        for train_index in range(epoch_size):\n",
    "            yield train_index\n",
    "            \n",
    "def random_trainer(N, k=None):\n",
    "    \"\"\"Yields N randomly chosen training examples\"\"\"\n",
    "    \n",
    "    for _ in range(N):\n",
    "        yield np.random.randint(0, epoch_size)\n",
    "        \n",
    "def random_minibatch_trainer(N, k):\n",
    "    \"\"\"Yields N/k minibatches of size k\"\"\"\n",
    "    \n",
    "    for _ in range(N/k):\n",
    "        yield np.random.choice(epoch_size, size=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call `train_sgd` to train on `X_train`, `y_train`. To verify that things work, train on 100,000 examples or so to start (with any of the above schedules). This shouldn't take more than a couple minutes, and you should get a mean cross-entropy loss around 0.4.\n",
    "\n",
    "Now, if this works well, it's time for production! You have three tasks here:\n",
    "\n",
    "1. Train a good model\n",
    "2. Plot a learning curve (cost vs. # of iterations)\n",
    "3. Use your best model to predict the test set\n",
    "\n",
    "You should train on the `train` data and evaluate performance on the `dev` set. The `test` data we provided has only dummy labels (everything is `O`); we'll compare your predictions to the true labels at grading time. \n",
    "\n",
    "Scroll down to section (f) for the evaluation code.\n",
    "\n",
    "We don't expect you to spend too much time doing an exhaustive search here; the default parameters should work well, although you can certainly do better. Try to achieve an F1 score of at least 76% on the dev set, as reported by `eval_performance`.\n",
    "\n",
    "Feel free to create new cells and write new code here, including new functions (helpers and otherwise) in `nerwindow.py`. When you have a good model, follow the instructions below to make predictions on the test set.\n",
    "\n",
    "A strong model may require 10-20 passes (or equivalent number of random samples) through the training set and could take 20 minutes or more to train - but it's also possible to be much, much faster!\n",
    "\n",
    "Things you may want to tune:\n",
    "- `alpha` (including using an \"annealing\" schedule to decrease the learning rate over time)\n",
    "- training schedule and minibatch size\n",
    "- regularization strength\n",
    "- hidden layer dimension\n",
    "- width of context window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox: Build a Good Model by Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203621"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Evaluation = namedtuple('Evaluation', ['alpha', 'minibatch_size', 'reg', 'num_hidden', 'window_width', 'counter', 'cost', 'clf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_size = len(y_train)\n",
    "\n",
    "def evaluate(alpha=.01, minibatch_size=5, reg=.001, num_hidden=100, window_width=3, trainer=random_minibatch_trainer):\n",
    "    # Instantiate windowed model with hyperparameter set\n",
    "    clf = WindowMLP(wv, windowsize=windowsize, dims=[None, num_hidden, 5], reg=reg, alpha=alpha)\n",
    "    \n",
    "    # Train the model\n",
    "    random.seed(10) # do not change this!\n",
    "    num_costs_desired = 20\n",
    "    costevery = epoch_size / (num_costs_desired*minibatch_size)\n",
    "    costs = clf.train_sgd(X_train, y_train,\n",
    "                          trainer(**{'N': epoch_size*6, 'k': minibatch_size}),\n",
    "                          printevery=costevery*2, costevery=costevery)\n",
    "    \n",
    "    for counter, cost in costs:\n",
    "        yield Evaluation(alpha, minibatch_size, reg, num_hidden, window_width, counter, cost, clf)\n",
    "\n",
    "def evaluations_generator(param, values):\n",
    "    for value in values:\n",
    "        yield list(evaluate(**{param: value}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nerwindow import eval_performance, full_report\n",
    "\n",
    "def hyperparam_search(param, values):\n",
    "    evaluations = list(evaluations_generator(param=param, values=values))\n",
    "\n",
    "    evaluations = [eval for eval_list in evaluations for eval in eval_list]\n",
    "\n",
    "    df = pd.DataFrame(evaluations, columns=Evaluation._fields)\n",
    "\n",
    "    df[[param, 'cost']].groupby(param).plot()\n",
    "\n",
    "    # Get classifier and evaluate\n",
    "    clf = df.iloc[-1].clf\n",
    "    predictions = clf.predict(X_dev)\n",
    "    full_report(y_dev, predictions, tagnames)\n",
    "    eval_performance(y_dev, predictions, tagnames)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  [2036]: mean loss 0.377176\n",
      "  Seen 4072 in 14.57 s\n",
      "  [4072]: mean loss 0.352735\n",
      "  [6108]: mean loss 0.343024\n",
      "  Seen 8144 in 29.08 s\n",
      "  [8144]: mean loss 0.32611\n",
      "  [10180]: mean loss 0.311012\n",
      "  Seen 12216 in 43.58 s\n",
      "  [12216]: mean loss 0.30548\n",
      "  [14252]: mean loss 0.298538\n",
      "  Seen 16288 in 58.08 s\n",
      "  [16288]: mean loss 0.294182\n",
      "  [18324]: mean loss 0.291829\n",
      "  Seen 20360 in 72.57 s\n",
      "  [20360]: mean loss 0.291276\n",
      "  [22396]: mean loss 0.289381\n",
      "  Seen 24432 in 87.02 s\n",
      "  [24432]: mean loss 0.272152\n",
      "  [26468]: mean loss 0.267267\n",
      "  Seen 28504 in 101.51 s\n",
      "  [28504]: mean loss 0.260738\n",
      "  [30540]: mean loss 0.259266\n",
      "  Seen 32576 in 115.98 s\n",
      "  [32576]: mean loss 0.253774\n",
      "  [34612]: mean loss 0.248754\n",
      "  Seen 36648 in 130.43 s\n",
      "  [36648]: mean loss 0.241621\n",
      "  [38684]: mean loss 0.241458\n",
      "  Seen 40720 in 144.85 s\n",
      "  [40720]: mean loss 0.236385\n",
      "  [42756]: mean loss 0.236458\n",
      "  Seen 44792 in 159.28 s\n",
      "  [44792]: mean loss 0.23184\n",
      "  [46828]: mean loss 0.225704\n",
      "  Seen 48864 in 173.72 s\n",
      "  [48864]: mean loss 0.224368\n",
      "  [50900]: mean loss 0.222458\n",
      "  Seen 52936 in 188.12 s\n",
      "  [52936]: mean loss 0.216766\n",
      "  [54972]: mean loss 0.217408\n",
      "  Seen 57008 in 202.54 s\n",
      "  [57008]: mean loss 0.21114\n",
      "  [59044]: mean loss 0.222905\n",
      "  Seen 61080 in 216.96 s\n",
      "  [61080]: mean loss 0.20217\n",
      "  [63116]: mean loss 0.203719\n",
      "  Seen 65152 in 231.41 s\n",
      "  [65152]: mean loss 0.20006\n",
      "  [67188]: mean loss 0.195383\n",
      "  Seen 69224 in 245.83 s\n",
      "  [69224]: mean loss 0.194841\n",
      "  [71260]: mean loss 0.199188\n",
      "  Seen 73296 in 260.25 s\n",
      "  [73296]: mean loss 0.196147\n",
      "  [75332]: mean loss 0.190695\n",
      "  Seen 77368 in 274.66 s\n",
      "  [77368]: mean loss 0.191059\n",
      "  [79404]: mean loss 0.18472\n",
      "  Seen 81440 in 289.12 s\n",
      "  [81440]: mean loss 0.187266\n",
      "  [83476]: mean loss 0.188222\n",
      "  Seen 85512 in 303.64 s\n",
      "  [85512]: mean loss 0.184634\n",
      "  [87548]: mean loss 0.175378\n",
      "  Seen 89584 in 318.11 s\n",
      "  [89584]: mean loss 0.173939\n",
      "  [91620]: mean loss 0.176308\n",
      "  Seen 93656 in 332.44 s\n",
      "  [93656]: mean loss 0.177296\n",
      "  [95692]: mean loss 0.171652\n",
      "  Seen 97728 in 346.71 s\n",
      "  [97728]: mean loss 0.168157\n",
      "  [99764]: mean loss 0.168634\n",
      "  Seen 101800 in 361.00 s\n",
      "  [101800]: mean loss 0.179673\n",
      "  [103836]: mean loss 0.167272\n",
      "  Seen 105872 in 375.26 s\n",
      "  [105872]: mean loss 0.166726\n",
      "  [107908]: mean loss 0.165894\n",
      "  Seen 109944 in 389.55 s\n",
      "  [109944]: mean loss 0.163655\n",
      "  [111980]: mean loss 0.171428\n",
      "  Seen 114016 in 403.88 s\n",
      "  [114016]: mean loss 0.161463\n",
      "  [116052]: mean loss 0.156949\n",
      "  Seen 118088 in 418.20 s\n",
      "  [118088]: mean loss 0.157005\n",
      "  [120124]: mean loss 0.156874\n",
      "  Seen 122160 in 432.49 s\n",
      "  [122160]: mean loss 0.159372\n",
      "  [124196]: mean loss 0.164487\n",
      "  Seen 126232 in 446.77 s\n",
      "  [126232]: mean loss 0.164235\n",
      "  [128268]: mean loss 0.15753\n",
      "  Seen 130304 in 461.08 s\n",
      "  [130304]: mean loss 0.155193\n",
      "  [132340]: mean loss 0.148216\n",
      "  Seen 134376 in 475.33 s\n",
      "  [134376]: mean loss 0.153127\n",
      "  [136412]: mean loss 0.146899\n",
      "  Seen 138448 in 489.55 s\n",
      "  [138448]: mean loss 0.158538\n",
      "  [140484]: mean loss 0.145128\n",
      "  Seen 142520 in 503.81 s\n",
      "  [142520]: mean loss 0.161693\n",
      "  [144556]: mean loss 0.147257\n",
      "  Seen 146592 in 518.03 s\n",
      "  [146592]: mean loss 0.14823\n",
      "  [148628]: mean loss 0.143503\n",
      "  Seen 150664 in 532.30 s\n",
      "  [150664]: mean loss 0.144244\n",
      "  [152700]: mean loss 0.139396\n",
      "  Seen 154736 in 546.64 s\n",
      "  [154736]: mean loss 0.143931\n",
      "  [156772]: mean loss 0.143183\n",
      "  Seen 158808 in 561.09 s\n",
      "  [158808]: mean loss 0.140154\n",
      "  [160844]: mean loss 0.139334\n",
      "  Seen 162880 in 575.45 s\n",
      "  [162880]: mean loss 0.139641\n",
      "  [164916]: mean loss 0.137282\n",
      "  Seen 166952 in 589.85 s\n",
      "  [166952]: mean loss 0.137373\n",
      "  [168988]: mean loss 0.134963\n",
      "  Seen 171024 in 604.23 s\n",
      "  [171024]: mean loss 0.136562\n",
      "  [173060]: mean loss 0.13737\n",
      "  Seen 175096 in 618.65 s\n",
      "  [175096]: mean loss 0.133131\n",
      "  [177132]: mean loss 0.145312\n",
      "  Seen 179168 in 633.06 s\n",
      "  [179168]: mean loss 0.135528\n",
      "  [181204]: mean loss 0.129908\n",
      "  Seen 183240 in 647.40 s\n",
      "  [183240]: mean loss 0.142294\n",
      "  [185276]: mean loss 0.133199\n",
      "  Seen 187312 in 661.77 s\n",
      "  [187312]: mean loss 0.130657\n",
      "  [189348]: mean loss 0.131844\n",
      "  Seen 191384 in 676.13 s\n",
      "  [191384]: mean loss 0.131104\n",
      "  [193420]: mean loss 0.12765\n",
      "  Seen 195456 in 690.69 s\n",
      "  [195456]: mean loss 0.127438\n",
      "  [197492]: mean loss 0.129867\n",
      "  Seen 199528 in 705.14 s\n",
      "  [199528]: mean loss 0.129015\n",
      "  [201564]: mean loss 0.130117\n",
      "  Seen 203600 in 719.61 s\n",
      "  [203600]: mean loss 0.128891\n",
      "  [205636]: mean loss 0.127838\n",
      "  Seen 207672 in 733.97 s\n",
      "  [207672]: mean loss 0.12741\n",
      "  [209708]: mean loss 0.123911\n",
      "  Seen 211744 in 748.35 s\n",
      "  [211744]: mean loss 0.124933\n",
      "  [213780]: mean loss 0.127355\n",
      "  Seen 215816 in 762.69 s\n",
      "  [215816]: mean loss 0.122101\n",
      "  [217852]: mean loss 0.123395\n",
      "  Seen 219888 in 777.03 s\n",
      "  [219888]: mean loss 0.125245\n",
      "  [221924]: mean loss 0.122372\n",
      "  Seen 223960 in 791.37 s\n",
      "  [223960]: mean loss 0.118475\n",
      "  [225996]: mean loss 0.119692\n",
      "  Seen 228032 in 805.72 s\n",
      "  [228032]: mean loss 0.124711\n",
      "  [230068]: mean loss 0.124428\n",
      "  Seen 232104 in 820.11 s\n",
      "  [232104]: mean loss 0.121373\n",
      "  [234140]: mean loss 0.121684\n",
      "  Seen 236176 in 834.47 s\n",
      "  [236176]: mean loss 0.120428\n",
      "  [238212]: mean loss 0.118821\n",
      "  Seen 240248 in 848.86 s\n",
      "  [240248]: mean loss 0.116384\n",
      "  [242284]: mean loss 0.118998\n",
      "  Seen 244320 in 863.16 s\n",
      "  [244320]: mean loss 0.117865\n",
      "  [244345]: mean loss 0.122189\n",
      "SGD complete: 244345 examples in 874.23 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.96      0.99      0.97     42759\n",
      "        LOC       0.81      0.86      0.83      2094\n",
      "       MISC       0.88      0.64      0.74      1268\n",
      "        ORG       0.76      0.52      0.62      2092\n",
      "        PER       0.89      0.78      0.83      3149\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  83.57%\n",
      "Mean recall:     71.67%\n",
      "Mean F1:         76.68%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAFkCAYAAAC0KZhSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqYzhQJIklfn/SB3WxVHxtFkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe5f9eb8490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = hyperparam_search('alpha', values=(.01,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e): Plot Learning Curves\n",
    "The `train_sgd` function returns a list of points `(counter, cost)` giving the mean loss after that number of SGD iterations.\n",
    "\n",
    "If the model is taking too long you can cut it off by going to *Kernel->Interrupt* in the IPython menu; `train_sgd` will return the training curve so-far, and you can restart without losing your training progress.\n",
    "\n",
    "Make two plots:\n",
    "\n",
    "- Learning curve using `reg = 0.001`, and comparing the effect of changing the learning rate: run with `alpha = 0.01` and `alpha = 0.1`. Use minibatches of size 5, and train for 10,000 minibatches with `costevery=200`. Be sure to scale up your counts (x-axis) to reflect the batch size. What happens if the model tries to learn too fast? Explain why this occurs, based on the relation of SGD to the true objective.\n",
    "\n",
    "- Learning curve for your best model (print the hyperparameters in the title), as trained using your best schedule. Set `costevery` so that you get at least 100 points to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1        2036\n",
       "2        4072\n",
       "3        6108\n",
       "4        8144\n",
       "5       10180\n",
       "6       12216\n",
       "7       14252\n",
       "8       16288\n",
       "9       18324\n",
       "10      20360\n",
       "11      22396\n",
       "12      24432\n",
       "13      26468\n",
       "14      28504\n",
       "15      30540\n",
       "16      32576\n",
       "17      34612\n",
       "18      36648\n",
       "19      38684\n",
       "20      40720\n",
       "21      42756\n",
       "22      44792\n",
       "23      46828\n",
       "24      48864\n",
       "25      50900\n",
       "26      52936\n",
       "27      54972\n",
       "28      57008\n",
       "29      59044\n",
       "        ...  \n",
       "92     187312\n",
       "93     189348\n",
       "94     191384\n",
       "95     193420\n",
       "96     195456\n",
       "97     197492\n",
       "98     199528\n",
       "99     201564\n",
       "100    203600\n",
       "101    205636\n",
       "102    207672\n",
       "103    209708\n",
       "104    211744\n",
       "105    213780\n",
       "106    215816\n",
       "107    217852\n",
       "108    219888\n",
       "109    221924\n",
       "110    223960\n",
       "111    225996\n",
       "112    228032\n",
       "113    230068\n",
       "114    232104\n",
       "115    234140\n",
       "116    236176\n",
       "117    238212\n",
       "118    240248\n",
       "119    242284\n",
       "120    244320\n",
       "121    244345\n",
       "Name: counter, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['counter', 'cost']].counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts, costs = list(df.counter), list(df.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = df.iloc[-1].clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGJCAYAAABy9cILAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe5f9e08c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Plot your best learning curve here\n",
    "# counts, costs = zip(*traincurvebest)\n",
    "figure(figsize=(6,4))\n",
    "plot(5*array(counts), costs, color='b', marker='o', linestyle='-')\n",
    "title(r\"Learning Curve ($\\alpha$=%g, $\\lambda$=%g)\" % (clf.alpha, clf.lreg))\n",
    "xlabel(\"SGD Iterations\"); ylabel(r\"Average $J(\\theta)$\"); \n",
    "ylim(ymin=0, ymax=max(1.1*max(costs),3*min(costs)));\n",
    "ylim(0,0.5)\n",
    "\n",
    "# Don't change this filename!\n",
    "savefig(\"ner.learningcurve.best.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainingcurve1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-a22c02fc96b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrainingcurve1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"$\\alpha=0.01$\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrainingcurve2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainingcurve1' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe5f4366b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Plot comparison of learning rates here\n",
    "# feel free to change the code below\n",
    "\n",
    "figure(figsize=(6,4))\n",
    "counts, costs = zip(*trainingcurve1)\n",
    "plot(5*array(counts), costs, color='b', marker='o', linestyle='-', label=r\"$\\alpha=0.01$\")\n",
    "counts, costs = zip(*trainingcurve2)\n",
    "plot(5*array(counts), costs, color='g', marker='o', linestyle='-', label=r\"$\\alpha=0.1$\")\n",
    "title(r\"Learning Curve ($\\lambda=0.01$, minibatch k=5)\")\n",
    "xlabel(\"SGD Iterations\"); ylabel(r\"Average $J(\\theta)$\"); \n",
    "ylim(ymin=0, ymax=max(1.1*max(costs),3*min(costs)));\n",
    "legend()\n",
    "\n",
    "# Don't change this filename\n",
    "savefig(\"ner.learningcurve.comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f): Evaluating your model\n",
    "Evaluate the model on the dev set using your `predict` function, and compute performance metrics below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict labels on the dev set\n",
    "yp = clf.predict(X_dev)\n",
    "# Save predictions to a file, one per line\n",
    "ner.save_predictions(yp, \"dev.predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.96      0.99      0.97     42759\n",
      "        LOC       0.81      0.86      0.83      2094\n",
      "       MISC       0.88      0.64      0.74      1268\n",
      "        ORG       0.76      0.52      0.62      2092\n",
      "        PER       0.89      0.78      0.83      3149\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  83.57%\n",
      "Mean recall:     71.67%\n",
      "Mean F1:         76.68%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(83.570364287709268, 71.67267232360804, 76.677326619656824)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nerwindow import full_report, eval_performance\n",
    "full_report(y_dev, yp, tagnames) # full report, helpful diagnostics\n",
    "eval_performance(y_dev, yp, tagnames) # performance: optimize this F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save your predictions on the test set for us to evaluate\n",
    "# IMPORTANT: make sure X_test is exactly as loaded \n",
    "# from du.docs_to_windows, so that your predictions \n",
    "# line up with ours.\n",
    "yptest = clf.predict(X_test)\n",
    "ner.save_predictions(yptest, \"test.predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part [1.1]: Probing neuron responses\n",
    "\n",
    "You might have seen some results from computer vision where the individual neurons learn to detect edges, shapes, or even [cat faces](http://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html). We're going to do the same for language.\n",
    "\n",
    "Recall that each \"neuron\" is essentially a logistic regression unit, with weights corresponding to rows of the corresponding matrix. So, if we have a hidden layer of dimension 100, then we can think of our matrix $W \\in \\mathbb{R}^{100 x 150}$ as representing 100 hidden neurons each with weights `W[i,:]` and bias `b1[i]`.\n",
    "\n",
    "### (a): Hidden Layer, Center Word\n",
    "For now, let's just look at the center word, and ignore the rest of the window. This corresponds to columns `W[:,50:100]`, although this could change if you altered the window size for your model. For each neuron, find the top 10 words that it responds to, as measured by the dot product between `W[i,50:100]` and `L[j]`. Use the provided code to print these words and their scores for 5 neurons of your choice. In your writeup, briefly describe what you notice here.\n",
    "\n",
    "The `num_to_word` dictionary, loaded earlier, may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron 1\n",
      "[0]: (0.771) going\n",
      "[1]: (0.776) unlikely\n",
      "[2]: (0.789) worth\n",
      "[3]: (0.791) shown\n",
      "[4]: (0.796) n't\n",
      "[5]: (0.802) situated\n",
      "[6]: (0.809) so\n",
      "[7]: (0.819) quite\n",
      "[8]: (0.849) fairly\n",
      "[9]: (0.905) very\n",
      "\n",
      "Neuron 3\n",
      "[0]: (0.053) scores\n",
      "[1]: (0.054) wife\n",
      "[2]: (0.054) instance\n",
      "[3]: (0.054) initial\n",
      "[4]: (0.055) extra\n",
      "[5]: (0.055) grandfather\n",
      "[6]: (0.055) </s>\n",
      "[7]: (0.058) there\n",
      "[8]: (0.061) why\n",
      "[9]: (0.063) example\n",
      "\n",
      "Neuron 4\n",
      "[0]: (0.020) worse\n",
      "[1]: (0.021) announcement\n",
      "[2]: (0.021) bonus\n",
      "[3]: (0.021) extra\n",
      "[4]: (0.021) scores\n",
      "[5]: (0.022) example\n",
      "[6]: (0.022) why\n",
      "[7]: (0.023) [\n",
      "[8]: (0.023) instance\n",
      "[9]: (0.025) estimated\n",
      "\n",
      "Neuron 6\n",
      "[0]: (1.528) scores\n",
      "[1]: (1.562) index\n",
      "[2]: (1.570) example\n",
      "[3]: (1.581) series\n",
      "[4]: (1.638) division\n",
      "[5]: (1.666) outcome\n",
      "[6]: (1.752) open\n",
      "[7]: (1.810) cup\n",
      "[8]: (1.822) explanation\n",
      "[9]: (1.853) score\n",
      "\n",
      "Neuron 8\n",
      "[0]: (0.257) norway\n",
      "[1]: (0.258) ireland\n",
      "[2]: (0.261) littoral\n",
      "[3]: (0.261) shore\n",
      "[4]: (0.263) india\n",
      "[5]: (0.267) uk\n",
      "[6]: (0.269) asia\n",
      "[7]: (0.271) london\n",
      "[8]: (0.279) wales\n",
      "[9]: (0.296) states\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recommended function to print scores\n",
    "# scores = list of float\n",
    "# words = list of str\n",
    "def print_scores(scores, words):\n",
    "    for i in range(len(scores)):\n",
    "        print \"[%d]: (%.03f) %s\" % (i, scores[i], words[i])\n",
    "\n",
    "scores = np.dot(clf.params.W[:, 50:100], clf.sparams.L.T)\n",
    "topwords_index_lists = np.argsort(scores, axis=1)[:, -10:]\n",
    "\n",
    "def convert_to_words():\n",
    "    for topwords_index_list in topwords_index_lists:\n",
    "        yield [num_to_word[word_index] for word_index in topwords_index_list]\n",
    "        \n",
    "topwords = list(convert_to_words())\n",
    "topscores = np.sort(scores, axis=1)[:, -10:]\n",
    "\n",
    "neurons = [1,3,4,6,8] # change this to your chosen neurons\n",
    "for i in neurons:\n",
    "    print \"Neuron %d\" % i\n",
    "    print_scores(topscores[i], topwords[i])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b): Model Output, Center Word\n",
    "Now, let's do the same for the output layer. Here we only have 5 neurons, one for each class. `O` isn't very interesting, but let's look at the other four.\n",
    "\n",
    "Here things get a little more complicated: since we take a softmax, we can't just look at the neurons separately. An input could cause several of these neurons to all have a strong response, so we really need to compute the softmax output and find the strongest inputs for each class.\n",
    "\n",
    "As before, let's consider only the center word (`W[:,50:100]`). For each class `ORG`, `PER`, `LOC`, and `MISC`, find the input words that give the highest probability $P(\\text{class}\\ |\\ \\text{word})$.\n",
    "\n",
    "You'll need to do the full feed-forward computation here - for efficiency, try to express this as a matrix operation on $L$. This is the same feed-forward computation as used to predict probabilities, just with $W$ replaced by `W[:,50:100]`.\n",
    "\n",
    "As with the hidden-layer neurons, print the top 10 words and their corresponding class probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output neuron 1: LOC\n",
      "[0]: (0.970) headland\n",
      "[1]: (0.974) korea\n",
      "[2]: (0.975) russia\n",
      "[3]: (0.976) england\n",
      "[4]: (0.980) britain\n",
      "[5]: (0.981) pakistan\n",
      "[6]: (0.983) norway\n",
      "[7]: (0.984) egypt\n",
      "[8]: (0.986) germany\n",
      "[9]: (0.988) italy\n",
      "\n",
      "Output neuron 2: MISC\n",
      "[0]: (0.981) egyptian\n",
      "[1]: (0.987) olympic\n",
      "[2]: (0.988) turkish\n",
      "[3]: (0.988) cup\n",
      "[4]: (0.989) belgian\n",
      "[5]: (0.989) english\n",
      "[6]: (0.991) danish\n",
      "[7]: (0.993) german\n",
      "[8]: (0.994) israeli\n",
      "[9]: (0.997) italian\n",
      "\n",
      "Output neuron 3: ORG\n",
      "[0]: (0.978) &\n",
      "[1]: (0.979) co\n",
      "[2]: (0.980) non-profit\n",
      "[3]: (0.981) ajax\n",
      "[4]: (0.983) charity\n",
      "[5]: (0.983) reuters\n",
      "[6]: (0.989) corp\n",
      "[7]: (0.990) arts\n",
      "[8]: (0.991) inc\n",
      "[9]: (0.992) commons\n",
      "\n",
      "Output neuron 4: PER\n",
      "[0]: (0.995) jackson\n",
      "[1]: (0.996) johnny\n",
      "[2]: (0.996) jason\n",
      "[3]: (0.996) carter\n",
      "[4]: (0.996) claw\n",
      "[5]: (0.997) dizzy\n",
      "[6]: (0.997) kelly\n",
      "[7]: (0.998) owl\n",
      "[8]: (0.998) jr.\n",
      "[9]: (0.999) remainder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from softmax import softmax_vectorized\n",
    "\n",
    "# Forward pass\n",
    "hidden = np.tanh(np.dot(clf.params.W[:, 50:100], clf.sparams.L.T) + clf.params.b1.reshape(100, 1))\n",
    "scores = np.dot(clf.params.U, hidden) + clf.params.b2.reshape(5, 1)\n",
    "probs = softmax_vectorized(scores)\n",
    "\n",
    "top_probs = np.sort(probs)[:, -10:]\n",
    "\n",
    "def generate_words():\n",
    "    for probs_list in np.argsort(probs)[:, -10:]:\n",
    "        yield [num_to_word[top_prob] for top_prob in probs_list]\n",
    "        \n",
    "topwords = list(generate_words())\n",
    "\n",
    "for i in range(1,5):\n",
    "    print \"Output neuron %d: %s\" % (i, num_to_tag[i])\n",
    "    print_scores(top_probs[i], topwords[i])\n",
    "    print \"\"\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c): Model Output, Preceding Word\n",
    "Now for one final task: let's look at the preceding word. Repeat the above analysis for the output layer, but use the first part of $W$, i.e. `W[:,:50]`.\n",
    "\n",
    "Describe what you see, and include these results in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output neuron 1: LOC\n",
      "[0]: (0.332) visited\n",
      "[1]: (0.342) lakes\n",
      "[2]: (0.344) dwelling\n",
      "[3]: (0.385) governed\n",
      "[4]: (0.407) adjacent\n",
      "[5]: (0.417) beneath\n",
      "[6]: (0.473) east\n",
      "[7]: (0.501) located\n",
      "[8]: (0.551) west\n",
      "[9]: (0.603) inhabited\n",
      "\n",
      "Output neuron 2: MISC\n",
      "[0]: (0.283) extinct\n",
      "[1]: (0.285) series\n",
      "[2]: (0.285) belong\n",
      "[3]: (0.310) zoo\n",
      "[4]: (0.322) cave\n",
      "[5]: (0.328) mystery\n",
      "[6]: (0.347) stadium\n",
      "[7]: (0.351) biblical\n",
      "[8]: (0.356) catholic\n",
      "[9]: (0.429) flock\n",
      "\n",
      "Output neuron 3: ORG\n",
      "[0]: (0.916) st\n",
      "[1]: (0.923) commons\n",
      "[2]: (0.925) grove\n",
      "[3]: (0.928) venture\n",
      "[4]: (0.929) alliance\n",
      "[5]: (0.931) enterprise\n",
      "[6]: (0.952) trust\n",
      "[7]: (0.953) corporation\n",
      "[8]: (0.964) v\n",
      "[9]: (0.972) &\n",
      "\n",
      "Output neuron 4: PER\n",
      "[0]: (0.990) j.\n",
      "[1]: (0.991) ode\n",
      "[2]: (0.991) p.\n",
      "[3]: (0.991) michael\n",
      "[4]: (0.992) jim\n",
      "[5]: (0.992) chris\n",
      "[6]: (0.992) alexander\n",
      "[7]: (0.994) m.\n",
      "[8]: (0.995) peter\n",
      "[9]: (0.997) jr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from softmax import softmax_vectorized\n",
    "\n",
    "# Forward pass\n",
    "hidden = np.tanh(np.dot(clf.params.W[:, :50], clf.sparams.L.T) + clf.params.b1.reshape(100, 1))\n",
    "scores = np.dot(clf.params.U, hidden) + clf.params.b2.reshape(5, 1)\n",
    "probs = softmax_vectorized(scores)\n",
    "\n",
    "top_probs = np.sort(probs)[:, -10:]\n",
    "\n",
    "def generate_words():\n",
    "    for probs_list in np.argsort(probs)[:, -10:]:\n",
    "        yield [num_to_word[top_prob] for top_prob in probs_list]\n",
    "        \n",
    "topwords = list(generate_words())\n",
    "\n",
    "for i in range(1,5):\n",
    "    print \"Output neuron %d: %s\" % (i, num_to_tag[i])\n",
    "    print_scores(top_probs[i], topwords[i])\n",
    "    print \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
